{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from brainlm_mae.modeling_brainlm import BrainLMForPretraining\n",
    "from utils.brainlm_trainer import BrainLMTrainer\n",
    "from utils.plots import plot_future_timepoint_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2023-07-17-19_00_00\"\n",
    "checkpoint_n = \"500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BrainLMForPretraining.from_pretrained(\n",
    "    f\"/home/ahf38/palmer_scratch/brainlm/training-runs/{model_name}/checkpoint-{checkpoint_n}\").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vit.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vit.embeddings.mask_ratio)\n",
    "print(model.vit.embeddings.config.mask_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this for matteo's branch, due to multiple train modes (auto-encoder, causal attention, predict last, etc)\n",
    "model.config.train_mode = \"auto_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Load Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKBioBank1000_Arrow_v4/Brain_Region_Coordinates\")\n",
    "print(coords_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "train_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/train_ukbiobank\")\n",
    "print(train_ds)\n",
    "val_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "print(val_ds)\n",
    "test_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/test_ukbiobank\")\n",
    "print(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[0]['Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ds = concatenate_datasets([train_ds, val_ds, test_ds])\n",
    "concat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = test_ds[500]\n",
    "print(example0['Filename'])\n",
    "print(example0['Patient ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"/home/mr2238/BrainLM/inference_plots/dataset_{dataset_v}/{model_name}_ckpt-{checkpoint_n}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = {\"train\": train_ds, \"val\": val_ds, \"test\": test_ds, \"concat\": concat_ds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Through Model, Pass Whole fMRI Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_of_interest_col_name = \"Gender\"\n",
    "recording_col_name = \"Voxelwise_RobustScaler_Normalized_Recording\"\n",
    "length = 200\n",
    "num_timepoints_per_voxel = model.config.num_timepoints_per_voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fmri(examples):\n",
    "    \"\"\"\n",
    "    Preprocessing function for dataset samples. This function is passed into Trainer as\n",
    "    a preprocessor which takes in one row of the loaded dataset and constructs a model\n",
    "    input sample according to the arguments which model.forward() expects.\n",
    "\n",
    "    The reason this function is defined inside on main() function is because we need\n",
    "    access to arguments such as cell_expression_vector_col_name.\n",
    "    \"\"\"\n",
    "    label = examples[variable_of_interest_col_name][0]\n",
    "    if math.isnan(label):\n",
    "        label = -1  # replace nans with -1\n",
    "    else:\n",
    "        label = int(label)\n",
    "    label = torch.tensor(label, dtype=torch.int64)\n",
    "    signal_vector = examples[recording_col_name][0]\n",
    "    signal_vector = torch.tensor(signal_vector, dtype=torch.float32)\n",
    "\n",
    "    # Choose random starting index, take window of moving_window_len points for each region\n",
    "    start_idx = randint(0, signal_vector.shape[0] - num_timepoints_per_voxel)\n",
    "    end_idx = start_idx + num_timepoints_per_voxel\n",
    "    signal_window = signal_vector[\n",
    "        start_idx:end_idx, :\n",
    "    ]  # [moving_window_len, num_voxels]\n",
    "    signal_window = torch.movedim(\n",
    "        signal_window, 0, 1\n",
    "    )  # --> [num_voxels, moving_window_len]\n",
    "\n",
    "    # Append signal values and coords\n",
    "    window_xyz_list = []\n",
    "    for brain_region_idx in range(signal_window.shape[0]):\n",
    "        # window_timepoint_list = torch.arange(0.0, 1.0, 1.0 / num_timepoints_per_voxel)\n",
    "\n",
    "        # Append voxel coordinates\n",
    "        xyz = torch.tensor(\n",
    "            [\n",
    "                coords_ds[brain_region_idx][\"X\"],\n",
    "                coords_ds[brain_region_idx][\"Y\"],\n",
    "                coords_ds[brain_region_idx][\"Z\"],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        window_xyz_list.append(xyz)\n",
    "    window_xyz_list = torch.stack(window_xyz_list)\n",
    "\n",
    "    # Add in key-value pairs for model inputs which CellLM is expecting in forward() function:\n",
    "    #  signal_vectors and xyz_vectors\n",
    "    #  These lists will be stacked into torch Tensors by collate() function (defined above).\n",
    "    examples[\"signal_vectors\"] = [signal_window]\n",
    "    examples[\"xyz_vectors\"] = [window_xyz_list]\n",
    "    examples[\"label\"] = [label]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    This function tells the dataloader how to stack a batch of examples from the dataset.\n",
    "    Need to stack gene expression vectors and maintain same argument names for model inputs\n",
    "    which CellLM is expecting in forward() function:\n",
    "        expression_vectors, sampled_gene_indices, and cell_indices\n",
    "    \"\"\"\n",
    "    signal_vectors = torch.stack(\n",
    "        [example[\"signal_vectors\"] for example in examples], dim=0\n",
    "    )\n",
    "    xyz_vectors = torch.stack([example[\"xyz_vectors\"] for example in examples])\n",
    "    labels = torch.stack([example[\"label\"] for example in examples])\n",
    "    \n",
    "    \n",
    "    # These inputs will go to model.forward(), names must match\n",
    "    return {\n",
    "        \"signal_vectors\": signal_vectors,\n",
    "        \"xyz_vectors\": xyz_vectors,\n",
    "        \"input_ids\": signal_vectors,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ds.set_transform(preprocess_fmri)\n",
    "test_ds.set_transform(preprocess_fmri)\n",
    "train_ds.set_transform(preprocess_fmri)\n",
    "val_ds.set_transform(preprocess_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_single = DataLoader(concat_ds, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(dataloader_single)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader_single))[\"signal_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Forward 1 sample through just the model encoder (model.vit) ---#\n",
    "with torch.no_grad():\n",
    "    example1 = next(iter(dataloader_single))\n",
    "    encoder_output = model.vit(\n",
    "        signal_vectors=example1[\"signal_vectors\"].to(device),\n",
    "        xyz_vectors=example1[\"xyz_vectors\"].to(device),\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"last_hidden_state:\", encoder_output.last_hidden_state.shape)\n",
    "# [batch_size, num_genes + 1 CLS token, hidden_dim]\n",
    "\n",
    "cls_token = encoder_output.last_hidden_state[:,0,:]\n",
    "print(cls_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete reconstruction of a batch of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batched = DataLoader(dataset_split[split], \n",
    "                               batch_size=batch_size, \n",
    "                               num_workers=6, \n",
    "                               collate_fn=collate_fn,\n",
    "                               pin_memory=True,\n",
    "                               drop_last=False,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_noise(x, seq_length, mask_ratio, ids_mask=None):\n",
    "    \"\"\"\n",
    "    Constructs a noise tensor which is used by model.vit.embeddings to mask tokens.\n",
    "    Giving ids_mask enables that every new call of construct_noise will return a tensor \n",
    "    that masks tokens that were not previously masked.\n",
    "\n",
    "    Args:\n",
    "        x:              tensor of shape [batch_size, num_voxels, num_timepoints_per_voxel]\n",
    "        seq_length:     length of tokens\n",
    "        mask_ratio:     ratio of tokens to mask\n",
    "        ids_mask:       previously masked tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # label dimensions of interest\n",
    "    batch_size = x.shape[0]\n",
    "    len_mask = int(mask_ratio * seq_length)\n",
    "    \n",
    "    # construct random only for not previously masked tokens\n",
    "    # add zeros to noise to force keep previously masked tokens\n",
    "    noise = torch.rand(batch_size, seq_length, device=x.device)\n",
    "    \n",
    "    if ids_mask != None:        \n",
    "        # force keep by setting noise to zero at prior masked indeces\n",
    "        noise.scatter_(index = ids_mask, dim = 1, value=0)\n",
    "\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    \n",
    "        ids_mask = torch.cat((ids_mask, ids_shuffle[:, (-1 * len_mask):]), dim=1)\n",
    "    else:\n",
    "        ids_mask = torch.argsort(noise, dim=1)[:, (-1 * len_mask):]  # ascend: small is keep, large is remove\n",
    "    \n",
    "    return noise, ids_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reconstruct whole recording for one example by repeatedly encoding/decoding\n",
    "example1 = next(iter(dataloader_batched))\n",
    "seq_length = (model.config.num_timepoints_per_voxel // model.config.timepoint_patching_size) * model.config.num_brain_voxels\n",
    "\n",
    "masked_tokens = 0\n",
    "ids_removed = None\n",
    "predictions = []\n",
    "masks = []\n",
    "x=example1[\"signal_vectors\"].to(device)\n",
    "while masked_tokens < seq_length:\n",
    "    \n",
    "    noise, ids_removed = construct_noise(x, seq_length, model.config.mask_ratio, ids_removed)\n",
    "    \n",
    "    # get predictions\n",
    "    out = model(signal_vectors=x, \n",
    "                xyz_vectors=example1[\"xyz_vectors\"].to(device),\n",
    "                noise=noise,\n",
    "               )\n",
    "    \n",
    "    # store predictions and masks\n",
    "    predictions.append(out.logits[0].detach())\n",
    "    print(\"Masked tokens this run at sample 0 and parcel 0:\", torch.nonzero(out.mask[0, 0, :]).tolist())\n",
    "    masks.append(out.mask)\n",
    "\n",
    "    \n",
    "    masked_tokens += out[\"mask\"][0, :].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(predictions, masks, mode=\"first\"):\n",
    "    '''\n",
    "    Aggregates all predictions according to masks.\n",
    "    Avoids adding a prediction twice if masked twice (will happen if num_tokens % (masked_ratio * num_tokens) != 0) \n",
    "    by taking the first prediction if mode = \"first\", or the average if mode = \"mean\". \n",
    "    '''\n",
    "    preds = torch.zeros(predictions[0].shape)\n",
    "    cum_mask = torch.zeros(masks[0].shape) # counts how many times particular token is masked\n",
    "    \n",
    "    for idx, p in enumerate(predictions):\n",
    "        cum_mask += masks[idx]\n",
    "        \n",
    "        if mode == \"first\":\n",
    "            masked_once = (cum_mask == 1) # keeps only tokens masked once\n",
    "            m = torch.eq(masked_once.long(), masks[idx]) # returns True for unmasked tokens and tokens masked for first time\n",
    "            m = m.long() * masks[idx] # returns only tokens masked for first time (mask[idx] will be zero for others)\n",
    "        else:\n",
    "            m = masks[idx]\n",
    "            \n",
    "        m = m.unsqueeze(-1).repeat(1, 1, 1, p.shape[-1])\n",
    "\n",
    "        preds += p * m \n",
    "\n",
    "    if mode == \"mean\":\n",
    "        preds = preds / cum_mask.unsqueeze(-1).repeat(1, 1, 1, p.shape[-1]) # there should not be a division by zero because all tokens masked at least once\n",
    "    \n",
    "    return (preds, cum_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, cum_mask = aggregate_predictions(predictions, masks, mode=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting one sample\n",
    "\n",
    "Visualize wholly reconstructed recording and ground truth. Plot UMAP and PCA over space as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcol\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_masked_pred_trends_one_sample(\n",
    "    pred_logits: np.array,\n",
    "    signal_vectors: np.array,\n",
    "    mask: np.array,\n",
    "    sample_idx: int,\n",
    "    node_idxs: np.array,\n",
    "    dataset_split: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to plot timeseries of model predictions as continuation of input data compared to\n",
    "    ground truth.\n",
    "    Args:\n",
    "        pred_logits:    numpy array of shape [batch_size, num_voxels, num_tokens, time_patch_preds]\n",
    "        signal_vectors: numpy array of shape [batch_size, num_voxels, num_tokens, time_patch_preds]\n",
    "        mask:           binary mask of shape [batch_size, num_voxels, num_tokens]\n",
    "        sample_idx:     index of sample to plot; one per figure\n",
    "        node_idxs:      indices of voxels to plot; affects how many columns in plot grid there will be\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=len(node_idxs), ncols=1, sharex=True, sharey=True)\n",
    "    fig.set_figwidth(25)\n",
    "    fig.set_figheight(3 * len(node_idxs))\n",
    "\n",
    "    batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "    # --- Plot Figure ---#\n",
    "    for row_idx, node_idx in enumerate(node_idxs):\n",
    "        ax = axes[row_idx]\n",
    "\n",
    "        input_data_vals = []\n",
    "        input_data_timepoints = []\n",
    "        for token_idx in range(signal_vectors.shape[2]):\n",
    "            input_data_vals += signal_vectors[sample_idx, node_idx, token_idx].tolist()\n",
    "            start_timepoint = time_patch_preds * token_idx\n",
    "            end_timepoint = start_timepoint + time_patch_preds\n",
    "            input_data_timepoints += list(range(start_timepoint, end_timepoint))\n",
    "\n",
    "            if mask[sample_idx, node_idx, token_idx] == 1:\n",
    "                model_pred_vals = pred_logits[sample_idx, node_idx, token_idx].tolist()\n",
    "                model_pred_timepoints = list(range(start_timepoint, end_timepoint))\n",
    "                ax.plot(\n",
    "                    model_pred_timepoints,\n",
    "                    model_pred_vals,\n",
    "                    marker=\".\",\n",
    "                    markersize=3,\n",
    "                    label=\"Masked Predictions\",\n",
    "                    color=\"orange\",\n",
    "                )\n",
    "\n",
    "        ax.plot(\n",
    "            input_data_timepoints,\n",
    "            input_data_vals,\n",
    "            marker=\".\",\n",
    "            markersize=3,\n",
    "            label=\"Input Data\",\n",
    "            color=\"green\",\n",
    "        )\n",
    "        ax.set_title(\"Sample {}, Parcel {}\".format(sample_idx, node_idx))\n",
    "        ax.axhline(y=0.0, color=\"gray\", linestyle=\"--\", markersize=2)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.9, 0.99))\n",
    "    plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])\n",
    "    fig.supxlabel(\"Timepoint\")\n",
    "    fig.supylabel(\"Prediction Value\")\n",
    "    plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(dataset_split))\n",
    "    plt.savefig(f\"{dir_name}reconstruct_whole_recording_{dataset_split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masked_pred_trends_one_sample(\n",
    "    preds,\n",
    "    example1[\"signal_vectors\"].reshape(preds.shape),\n",
    "    mask=torch.ones(preds.shape[:3]),\n",
    "    sample_idx=0,\n",
    "    node_idxs=[0, 100, 200],\n",
    "    dataset_split=split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one recording to map, transpose to do PCA and UMAP over time\n",
    "raw_rec = example1[\"signal_vectors\"][0].T\n",
    "pred_rec = preds[0].flatten(-2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply UMAP to raw recording\n",
    "reducer = umap.UMAP(random_state=42, verbose = True, n_components=3)\n",
    "embedding_raw = reducer.fit_transform(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to reconstructed recording\n",
    "reducer = umap.UMAP(random_state=42, verbose = True, n_components=3)\n",
    "embedding_pred = reducer.fit_transform(pred_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=embedding_pred.shape[1], ncols=1, sharex=True, sharey=False)\n",
    "fig.set_figwidth(25)\n",
    "fig.set_figheight(3 * embedding_pred.shape[1])\n",
    "\n",
    "# batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "# --- Plot Figure ---#\n",
    "for row_idx in range(embedding_pred.shape[1]):\n",
    "    ax = axes[row_idx]\n",
    "    data_parcels = list(range(embedding_pred.shape[0]))\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        embedding_pred[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Masked Predictions\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        embedding_raw[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Input Data\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.set_title(\"UMAP Coord {}\".format(row_idx))\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, 0.9))\n",
    "plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])\n",
    "fig.supxlabel(\"Timepoint\")\n",
    "fig.supylabel(\"UMAP value\")\n",
    "plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(split))\n",
    "plt.savefig(f\"{dir_name}reconstruct_umap_{split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these two representations a translated vertically by different amounts, or flipped along horizontal axis. TODO: figure out a way (if there is one) to have them in the same line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "pca_pred = PCA(n_components=n_components)\n",
    "pca_raw = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pred.fit(pred_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_raw.fit(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reduced = pca_pred.transform(pred_rec)\n",
    "raw_reduced = pca_raw.transform(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=pred_reduced.shape[1], ncols=1, sharex=True, sharey=False)\n",
    "fig.set_figwidth(25)\n",
    "fig.set_figheight(3 * pred_reduced.shape[1])\n",
    "\n",
    "# batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "# --- Plot Figure ---#\n",
    "for row_idx in range(pred_reduced.shape[1]):\n",
    "    ax = axes[row_idx]\n",
    "    data_parcels = list(range(pred_reduced.shape[0]))\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        pred_reduced[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Masked Predictions\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        raw_reduced[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Input Data\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.set_title(\"PCA Coord {}\".format(row_idx + 1))\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(0.9, 0.93))\n",
    "plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.93])\n",
    "fig.supxlabel(\"Timepoint\")\n",
    "fig.supylabel(\"PCA Value\")\n",
    "plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(split))\n",
    "plt.savefig(f\"{dir_name}reconstruct_pca_whole_recording_{split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 1) Average of a batch of recordings, 2) average of all recordings in a dataset split, 3) average of all recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
